{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialisation of Spark environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0.1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "if os.environ['SPARK_HOME'] is None:\n",
    "    # assuming, you installed Spark in your home directory:\n",
    "    sparkPath = \"/home/user/spark-3.0.1-bin-hadoop2.7\" #TODO: adjust this\n",
    "    os.environ['SPARK_HOME'] = sparkPath\n",
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "import re\n",
    "spark = SparkSession.builder.appName(\"DataFramesSparkSQL\").getOrCreate()\n",
    "print(spark.version)\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"WARN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download the file *'yellow_tripdata_2016-01_10k.csv'* from the OPAL section 'Exercises/Ãœbungen/Data'.\n",
    "More information about the dataset:\n",
    "- https://data.cityofnewyork.us/dataset/Yellow-Tripdata-2015-January-June/2yzn-sicd\n",
    "- http://www.nyc.gov/html/tlc/html/about/trip_record_data.shtml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of entries: 10000\n"
     ]
    }
   ],
   "source": [
    "dataFile = \"yellow_tripdata_2016-01_10k.csv\"\n",
    "taxiLineRDD = sc.textFile(dataFile)\n",
    "print(\"number of entries: {}\".format(taxiLineRDD.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VendorID,tpep_pickup_datetime,tpep_dropoff_datetime,passenger_count,trip_distance,pickup_longitude,pickup_latitude,RatecodeID,store_and_fwd_flag,dropoff_longitude,dropoff_latitude,payment_type,fare_amount,extra,mta_tax,tip_amount,tolls_amount,improvement_surcharge,total_amount\n",
      "2,2016-01-01 00:00:00,2016-01-01 00:00:00,2,1.10,-73.990371704101563,40.734695434570313,1,N,-73.981842041015625,40.732406616210937,2,7.5,0.5,0.5,0,0,0.3,8.8\n",
      "2,2016-01-01 00:00:00,2016-01-01 00:00:00,5,4.90,-73.980781555175781,40.729911804199219,1,N,-73.944473266601563,40.716678619384766,1,18,0.5,0.5,0,0,0.3,19.3\n"
     ]
    }
   ],
   "source": [
    "# inspect data\n",
    "for line in taxiLineRDD.take(3):\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Header is:\n",
      "VendorID,tpep_pickup_datetime,tpep_dropoff_datetime,passenger_count,trip_distance,pickup_longitude,pickup_latitude,RatecodeID,store_and_fwd_flag,dropoff_longitude,dropoff_latitude,payment_type,fare_amount,extra,mta_tax,tip_amount,tolls_amount,improvement_surcharge,total_amount\n"
     ]
    }
   ],
   "source": [
    "# obviously, there is a header\n",
    "header = taxiLineRDD.first()\n",
    "print('Header is:')\n",
    "print(header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'datetime.datetime'>\n"
     ]
    }
   ],
   "source": [
    "# we need to be able to handle date and times...\n",
    "from datetime import *\n",
    "from dateutil.parser import parse\n",
    "print(type(parse(\"2013-02-09 18:16:10\")))  # test it: We should have a datetime object then..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1: Measure the time for filtering and collecting. Explain the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "taxi header RDD:\n",
      "['VendorID,tpep_pickup_datetime,tpep_dropoff_datetime,passenger_count,trip_distance,pickup_longitude,pickup_latitude,RatecodeID,store_and_fwd_flag,dropoff_longitude,dropoff_latitude,payment_type,fare_amount,extra,mta_tax,tip_amount,tolls_amount,improvement_surcharge,total_amount']\n",
      "time for filtering:\n",
      "0.003462076187133789\n",
      "time for collecting:\n",
      "0.10451912879943848\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "time_before = time()\n",
    "taxiHeaderRDD = taxiLineRDD.filter(lambda item: \"VendorID\" in item)\n",
    "time_after_filter = time()\n",
    "collectedHeader = taxiHeaderRDD.collect()\n",
    "time_after_collect = time()\n",
    "print('taxi header RDD:')\n",
    "print(collectedHeader)\n",
    "print('time for filtering:')\n",
    "print(time_after_filter-time_before)\n",
    "print('time for collecting:')\n",
    "print(time_after_collect-time_after_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9999\n",
      "First line with data:\n",
      "2,2016-01-01 00:00:00,2016-01-01 00:00:00,1,10.54,-73.984550476074219,40.6795654296875,1,N,-73.950271606445313,40.788925170898438,1,33,0.5,0.5,0,0,0.3,34.3\n"
     ]
    }
   ],
   "source": [
    "# subtract works on datasets:\n",
    "taxiDataLineRDD = taxiLineRDD.subtract(taxiHeaderRDD)\n",
    "# taxiDataLineRDD could be created also directly:\n",
    "# taxiDataLineRDD = taxiLineRDD.filter(lambda item: \"VendorID\" not in item)\n",
    "print(taxiDataLineRDD.count())\n",
    "print('First line with data:')\n",
    "print(taxiDataLineRDD.first())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2: Compute transformation and action times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First line as a row:\n",
      "Row(ID='2', dateA='2016-01-01 00:00:00', dateB=datetime.datetime(2016, 1, 1, 0, 0), passenger=1, dist=10.54, fare='33', tip='0')\n",
      "time for mapping:\n",
      "0.0001499652862548828\n",
      "time for first:\n",
      "0.07567524909973145\n"
     ]
    }
   ],
   "source": [
    "# how long does it take to compute transformation and action results?\n",
    "time_before = time()\n",
    "# convert data types which we use later:\n",
    "taxiRowRDD = (taxiDataLineRDD.map(lambda item: item.split(\",\"))\n",
    "                .map(lambda item: Row(ID=item[0], dateA=item[1], dateB=parse(item[2]), passenger=int(item[3]), dist=float(item[4]), fare=item[12], tip=item[15] )\n",
    "                    ))\n",
    "time_after_map = time()\n",
    "firstRow = taxiRowRDD.first()\n",
    "time_after_first = time()\n",
    "print('First line as a row:')\n",
    "print(firstRow)\n",
    "print('time for mapping:')\n",
    "print(time_after_map-time_before)\n",
    "print('time for first:')\n",
    "print(time_after_first-time_after_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3: Create an overview over the data: Show the number of data rows, the types of the columns and a short excerpt of the data.\n",
    "Hint: Visit the documentation page for DataFrame's pyspark API\n",
    "\n",
    "https://spark.apache.org/docs/2.4.3/api/python/pyspark.sql.html#pyspark.sql.DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9999\n"
     ]
    }
   ],
   "source": [
    "taxiDF=spark.createDataFrame(taxiRowRDD)\n",
    "# show number of data rows\n",
    "print(taxiDF.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ID', 'string'),\n",
       " ('dateA', 'string'),\n",
       " ('dateB', 'timestamp'),\n",
       " ('passenger', 'bigint'),\n",
       " ('dist', 'double'),\n",
       " ('fare', 'string'),\n",
       " ('tip', 'string')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show data types\n",
    "taxiDF.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+-------------------+---------+-----+----+----+\n",
      "| ID|              dateA|              dateB|passenger| dist|fare| tip|\n",
      "+---+-------------------+-------------------+---------+-----+----+----+\n",
      "|  2|2016-01-01 00:00:00|2016-01-01 00:00:00|        1|10.54|  33|   0|\n",
      "|  2|2016-01-01 00:00:02|2016-01-01 00:11:08|        1| 3.21|11.5|   0|\n",
      "|  2|2016-01-01 00:00:03|2016-01-01 00:15:49|        6| 2.43|  12|3.99|\n",
      "|  1|2016-01-01 00:00:05|2016-01-01 00:14:27|        2|  2.2|  11| 1.5|\n",
      "|  2|2016-01-01 00:00:05|2016-01-01 00:07:17|        1| 0.54|   6|   0|\n",
      "|  2|2016-01-01 00:00:08|2016-01-01 00:03:24|        1| 0.69| 4.5|   0|\n",
      "|  1|2016-01-01 00:00:09|2016-01-01 00:19:03|        3|  5.3|  18|3.85|\n",
      "|  2|2016-01-01 00:00:10|2016-01-01 00:02:20|        1| 0.87| 4.5|   0|\n",
      "|  2|2016-01-01 00:00:12|2016-01-01 00:01:17|        1| 0.13|   3|   0|\n",
      "|  1|2016-01-01 00:00:17|2016-01-01 00:13:59|        1|  1.5|  10|3.35|\n",
      "|  1|2016-01-01 00:00:21|2016-01-01 00:17:30|        1|  0.7|  11|   0|\n",
      "|  1|2016-01-01 00:00:21|2016-01-01 00:32:09|        1|  5.4|24.5|   0|\n",
      "|  2|2016-01-01 00:00:22|2016-01-01 00:12:12|        1|  3.3|  12|2.66|\n",
      "|  2|2016-01-01 00:00:22|2016-01-01 00:13:37|        6| 2.94|  12|   0|\n",
      "|  2|2016-01-01 00:00:22|2016-01-01 00:13:58|        2| 5.23|17.5|   0|\n",
      "|  2|2016-01-01 00:00:32|2016-01-01 00:09:37|        6| 0.32|   7|   0|\n",
      "|  1|2016-01-01 00:00:39|2016-01-01 00:08:41|        1|  2.2|   9|   0|\n",
      "|  2|2016-01-01 00:00:41|2016-01-01 00:00:46|        5|  0.0|  20|   0|\n",
      "|  2|2016-01-01 00:00:50|2016-01-01 00:06:29|        6| 0.72| 5.5|1.36|\n",
      "|  2|2016-01-01 00:00:55|2016-01-01 00:28:34|        4|15.64|  44|   0|\n",
      "+---+-------------------+-------------------+---------+-----+----+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show a short excerpt\n",
    "taxiDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| ID|count|\n",
      "+---+-----+\n",
      "|  1| 4354|\n",
      "|  2| 5645|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# using groupBy\n",
    "taxiDF.groupBy(\"ID\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n",
      "| ID|count(1)|\n",
      "+---+--------+\n",
      "|  1|    4354|\n",
      "|  2|    5645|\n",
      "+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# equivalent method with an SQL query\n",
    "# Create a temporary view from the DataFrame\n",
    "taxiDF.registerTempTable(\"taxi\")\n",
    "sqlContext.sql(\"SELECT ID, COUNT(*) FROM taxi GROUP BY ID\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4: Count trips grouped by passengers. Find the minimal distance as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|passenger|count|\n",
      "+---------+-----+\n",
      "|        6|  351|\n",
      "|        5|  605|\n",
      "|        1| 6602|\n",
      "|        3|  539|\n",
      "|        2| 1631|\n",
      "|        4|  271|\n",
      "+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "taxiDF.groupBy(\"passenger\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+\n",
      "|passenger|min(dist)|\n",
      "+---------+---------+\n",
      "|        6|     0.25|\n",
      "|        5|      0.0|\n",
      "|        1|      0.0|\n",
      "|        3|      0.0|\n",
      "|        2|      0.0|\n",
      "|        4|     0.01|\n",
      "+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "taxiDF.groupBy(\"passenger\").min(\"dist\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Query**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|fare|\n",
      "+----+\n",
      "|  33|\n",
      "|  18|\n",
      "|24.5|\n",
      "|17.5|\n",
      "|  44|\n",
      "|40.5|\n",
      "|20.5|\n",
      "|  17|\n",
      "|22.5|\n",
      "|  18|\n",
      "|  52|\n",
      "|23.5|\n",
      "|29.5|\n",
      "|26.5|\n",
      "|  22|\n",
      "|  26|\n",
      "| 126|\n",
      "|32.5|\n",
      "|  37|\n",
      "|  31|\n",
      "+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Apply a SQL query\n",
    "query = \"SELECT fare FROM taxi WHERE dist>=5\"\n",
    "sqlContext.sql(query).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 5: Rewrite the previous statement without SQL, but with a functional statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|fare|\n",
      "+----+\n",
      "|  33|\n",
      "|  18|\n",
      "|24.5|\n",
      "|17.5|\n",
      "|  44|\n",
      "|40.5|\n",
      "|20.5|\n",
      "|  17|\n",
      "|22.5|\n",
      "|  18|\n",
      "|  52|\n",
      "|23.5|\n",
      "|29.5|\n",
      "|26.5|\n",
      "|  22|\n",
      "|  26|\n",
      "| 126|\n",
      "|32.5|\n",
      "|  37|\n",
      "|  31|\n",
      "+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "taxiDF.filter(taxiDF.dist>5).select(\"fare\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+-------------------+------------------+------------------+------------------+-----------------+\n",
      "|summary|                 ID|              dateA|         passenger|              dist|              fare|              tip|\n",
      "+-------+-------------------+-------------------+------------------+------------------+------------------+-----------------+\n",
      "|  count|               9999|               9999|              9999|              9999|              9999|             9999|\n",
      "|   mean| 1.5645564556455644|               null|1.7697769776977699|  3.06995699569957|11.937063706370637|1.504447444744474|\n",
      "| stddev|0.49583974447246415|               null| 1.369663409687648|3.3514113414719424| 9.643450090344786|2.349804623488286|\n",
      "|    min|                  1|2016-01-01 00:00:00|                 1|               0.0|              -2.5|                0|\n",
      "|    max|                  2|2016-01-29 09:20:52|                 6|             47.39|               9.8|             9.85|\n",
      "+-------+-------------------+-------------------+------------------+------------------+------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#compute summary statistics\n",
    "taxiDF.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 6: Find the distance for tips larger than $5  - Formulate a SQL query and apply it on the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "| dist|\n",
      "+-----+\n",
      "| 7.04|\n",
      "|19.22|\n",
      "| 6.82|\n",
      "|  6.1|\n",
      "|  7.9|\n",
      "| 10.0|\n",
      "|13.27|\n",
      "| 9.46|\n",
      "|  5.3|\n",
      "| 8.35|\n",
      "|  7.7|\n",
      "|12.94|\n",
      "|21.36|\n",
      "| 5.23|\n",
      "|14.84|\n",
      "|13.28|\n",
      "| 13.0|\n",
      "| 14.0|\n",
      "| 8.35|\n",
      "| 10.5|\n",
      "+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"SELECT dist FROM taxi WHERE tip>=5\"\n",
    "sqlContext.sql(query).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 7: Include the total cost into the DataFrame and build a new DataFrame object. Afterwards select all tips for distances > 30miles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after mapping transformation number of elements>\n",
      "Row(ID='2', dateA='2016-01-01 00:00:00', dateB=datetime.datetime(2016, 1, 1, 0, 0), passenger=1, dist=10.54, fare='33', tip='0', total='34.3')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(ID='1', dateA='2016-01-29 09:19:45', dateB=datetime.datetime(2016, 1, 29, 10, 18, 31), passenger=3, dist=42.0, fare='126', tip='0.01', total='142.35'),\n",
       " Row(ID='2', dateA='2016-01-02 01:04:23', dateB=datetime.datetime(2016, 1, 2, 2, 0, 52), passenger=1, dist=47.39, fare='139', tip='30.18', total='181.06'),\n",
       " Row(ID='1', dateA='2016-01-02 01:29:33', dateB=datetime.datetime(2016, 1, 2, 2, 28, 3), passenger=1, dist=45.5, fare='150', tip='0', total='176.3')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxiWithTotalRowRDD = (taxiDataLineRDD.map(lambda item: item.split(\",\"))\n",
    "                .map(lambda item: Row(ID=item[0], dateA=item[1], dateB=parse(item[2]), passenger=int(item[3]), dist=float(item[4]), fare=item[12], tip=item[15], total=item[18] )\n",
    "                    ))\n",
    "taxiWithTotalDF = spark.createDataFrame(taxiWithTotalRowRDD)\n",
    "print('after mapping transformation number of elements>')\n",
    "print(taxiWithTotalRowRDD.first())\n",
    "taxiWithTotalDF.filter(taxiWithTotalDF.dist > 30).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 8: Count the number of entries to verify the input and show the content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9999\n",
      "+---+-------------------+-------------------+---------+-----+----+----+-----+\n",
      "| ID|              dateA|              dateB|passenger| dist|fare| tip|total|\n",
      "+---+-------------------+-------------------+---------+-----+----+----+-----+\n",
      "|  2|2016-01-01 00:00:00|2016-01-01 00:00:00|        1|10.54|  33|   0| 34.3|\n",
      "|  2|2016-01-01 00:00:02|2016-01-01 00:11:08|        1| 3.21|11.5|   0| 12.8|\n",
      "|  2|2016-01-01 00:00:03|2016-01-01 00:15:49|        6| 2.43|  12|3.99|17.29|\n",
      "|  1|2016-01-01 00:00:05|2016-01-01 00:14:27|        2|  2.2|  11| 1.5| 13.8|\n",
      "|  2|2016-01-01 00:00:05|2016-01-01 00:07:17|        1| 0.54|   6|   0|  7.3|\n",
      "|  2|2016-01-01 00:00:08|2016-01-01 00:03:24|        1| 0.69| 4.5|   0|  5.8|\n",
      "|  1|2016-01-01 00:00:09|2016-01-01 00:19:03|        3|  5.3|  18|3.85|23.15|\n",
      "|  2|2016-01-01 00:00:10|2016-01-01 00:02:20|        1| 0.87| 4.5|   0|  5.8|\n",
      "|  2|2016-01-01 00:00:12|2016-01-01 00:01:17|        1| 0.13|   3|   0|  4.3|\n",
      "|  1|2016-01-01 00:00:17|2016-01-01 00:13:59|        1|  1.5|  10|3.35|14.65|\n",
      "|  1|2016-01-01 00:00:21|2016-01-01 00:17:30|        1|  0.7|  11|   0| 12.3|\n",
      "|  1|2016-01-01 00:00:21|2016-01-01 00:32:09|        1|  5.4|24.5|   0| 25.8|\n",
      "|  2|2016-01-01 00:00:22|2016-01-01 00:12:12|        1|  3.3|  12|2.66|15.96|\n",
      "|  2|2016-01-01 00:00:22|2016-01-01 00:13:37|        6| 2.94|  12|   0| 13.3|\n",
      "|  2|2016-01-01 00:00:22|2016-01-01 00:13:58|        2| 5.23|17.5|   0| 18.8|\n",
      "|  2|2016-01-01 00:00:32|2016-01-01 00:09:37|        6| 0.32|   7|   0|  8.3|\n",
      "|  1|2016-01-01 00:00:39|2016-01-01 00:08:41|        1|  2.2|   9|   0| 10.3|\n",
      "|  2|2016-01-01 00:00:41|2016-01-01 00:00:46|        5|  0.0|  20|   0| 20.8|\n",
      "|  2|2016-01-01 00:00:50|2016-01-01 00:06:29|        6| 0.72| 5.5|1.36| 8.16|\n",
      "|  2|2016-01-01 00:00:55|2016-01-01 00:28:34|        4|15.64|  44|   0| 45.3|\n",
      "+---+-------------------+-------------------+---------+-----+----+----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(taxiWithTotalDF.count())\n",
    "taxiWithTotalDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 9: Formulate a query to get total amount of trip for distances larger than 30 miles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+\n",
      "|sum(CAST(total AS DOUBLE))|\n",
      "+--------------------------+\n",
      "|                    499.71|\n",
      "+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "taxiWithTotalDF.registerTempTable(\"taxiWithTotal\")\n",
    "query = \"SELECT SUM(total) FROM taxiWithTotal WHERE dist>=30\"\n",
    "sqlContext.sql(query).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 10: Create a box-and-whisker plot of the numerical columns. What do these say about the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on DataFrame in module pyspark.sql.dataframe object:\n",
      "\n",
      "class DataFrame(pyspark.sql.pandas.map_ops.PandasMapOpsMixin, pyspark.sql.pandas.conversion.PandasConversionMixin)\n",
      " |  A distributed collection of data grouped into named columns.\n",
      " |  \n",
      " |  A :class:`DataFrame` is equivalent to a relational table in Spark SQL,\n",
      " |  and can be created using various functions in :class:`SparkSession`::\n",
      " |  \n",
      " |      people = spark.read.parquet(\"...\")\n",
      " |  \n",
      " |  Once created, it can be manipulated using the various domain-specific-language\n",
      " |  (DSL) functions defined in: :class:`DataFrame`, :class:`Column`.\n",
      " |  \n",
      " |  To select a column from the :class:`DataFrame`, use the apply method::\n",
      " |  \n",
      " |      ageCol = people.age\n",
      " |  \n",
      " |  A more concrete example::\n",
      " |  \n",
      " |      # To create DataFrame using SparkSession\n",
      " |      people = spark.read.parquet(\"...\")\n",
      " |      department = spark.read.parquet(\"...\")\n",
      " |  \n",
      " |      people.filter(people.age > 30).join(department, people.deptId == department.id) \\\n",
      " |        .groupBy(department.name, \"gender\").agg({\"salary\": \"avg\", \"age\": \"max\"})\n",
      " |  \n",
      " |  .. versionadded:: 1.3\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      DataFrame\n",
      " |      pyspark.sql.pandas.map_ops.PandasMapOpsMixin\n",
      " |      pyspark.sql.pandas.conversion.PandasConversionMixin\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __getattr__(self, name)\n",
      " |      Returns the :class:`Column` denoted by ``name``.\n",
      " |      \n",
      " |      >>> df.select(df.age).collect()\n",
      " |      [Row(age=2), Row(age=5)]\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  __getitem__(self, item)\n",
      " |      Returns the column as a :class:`Column`.\n",
      " |      \n",
      " |      >>> df.select(df['age']).collect()\n",
      " |      [Row(age=2), Row(age=5)]\n",
      " |      >>> df[ [\"name\", \"age\"]].collect()\n",
      " |      [Row(name='Alice', age=2), Row(name='Bob', age=5)]\n",
      " |      >>> df[ df.age > 3 ].collect()\n",
      " |      [Row(age=5, name='Bob')]\n",
      " |      >>> df[df[0] > 3].collect()\n",
      " |      [Row(age=5, name='Bob')]\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  __init__(self, jdf, sql_ctx)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  agg(self, *exprs)\n",
      " |      Aggregate on the entire :class:`DataFrame` without groups\n",
      " |      (shorthand for ``df.groupBy.agg()``).\n",
      " |      \n",
      " |      >>> df.agg({\"age\": \"max\"}).collect()\n",
      " |      [Row(max(age)=5)]\n",
      " |      >>> from pyspark.sql import functions as F\n",
      " |      >>> df.agg(F.min(df.age)).collect()\n",
      " |      [Row(min(age)=2)]\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  alias(self, alias)\n",
      " |      Returns a new :class:`DataFrame` with an alias set.\n",
      " |      \n",
      " |      :param alias: string, an alias name to be set for the :class:`DataFrame`.\n",
      " |      \n",
      " |      >>> from pyspark.sql.functions import *\n",
      " |      >>> df_as1 = df.alias(\"df_as1\")\n",
      " |      >>> df_as2 = df.alias(\"df_as2\")\n",
      " |      >>> joined_df = df_as1.join(df_as2, col(\"df_as1.name\") == col(\"df_as2.name\"), 'inner')\n",
      " |      >>> joined_df.select(\"df_as1.name\", \"df_as2.name\", \"df_as2.age\")                 .sort(desc(\"df_as1.name\")).collect()\n",
      " |      [Row(name='Bob', name='Bob', age=5), Row(name='Alice', name='Alice', age=2)]\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  approxQuantile(self, col, probabilities, relativeError)\n",
      " |      Calculates the approximate quantiles of numerical columns of a\n",
      " |      :class:`DataFrame`.\n",
      " |      \n",
      " |      The result of this algorithm has the following deterministic bound:\n",
      " |      If the :class:`DataFrame` has N elements and if we request the quantile at\n",
      " |      probability `p` up to error `err`, then the algorithm will return\n",
      " |      a sample `x` from the :class:`DataFrame` so that the *exact* rank of `x` is\n",
      " |      close to (p * N). More precisely,\n",
      " |      \n",
      " |        floor((p - err) * N) <= rank(x) <= ceil((p + err) * N).\n",
      " |      \n",
      " |      This method implements a variation of the Greenwald-Khanna\n",
      " |      algorithm (with some speed optimizations). The algorithm was first\n",
      " |      present in [[https://doi.org/10.1145/375663.375670\n",
      " |      Space-efficient Online Computation of Quantile Summaries]]\n",
      " |      by Greenwald and Khanna.\n",
      " |      \n",
      " |      Note that null values will be ignored in numerical columns before calculation.\n",
      " |      For columns only containing null values, an empty list is returned.\n",
      " |      \n",
      " |      :param col: str, list.\n",
      " |        Can be a single column name, or a list of names for multiple columns.\n",
      " |      :param probabilities: a list of quantile probabilities\n",
      " |        Each number must belong to [0, 1].\n",
      " |        For example 0 is the minimum, 0.5 is the median, 1 is the maximum.\n",
      " |      :param relativeError:  The relative target precision to achieve\n",
      " |        (>= 0). If set to zero, the exact quantiles are computed, which\n",
      " |        could be very expensive. Note that values greater than 1 are\n",
      " |        accepted but give the same result as 1.\n",
      " |      :return:  the approximate quantiles at the given probabilities. If\n",
      " |        the input `col` is a string, the output is a list of floats. If the\n",
      " |        input `col` is a list or tuple of strings, the output is also a\n",
      " |        list, but each element in it is a list of floats, i.e., the output\n",
      " |        is a list of list of floats.\n",
      " |      \n",
      " |      .. versionchanged:: 2.2\n",
      " |         Added support for multiple columns.\n",
      " |      \n",
      " |      .. versionadded:: 2.0\n",
      " |  \n",
      " |  cache(self)\n",
      " |      Persists the :class:`DataFrame` with the default storage level (`MEMORY_AND_DISK`).\n",
      " |      \n",
      " |      .. note:: The default storage level has changed to `MEMORY_AND_DISK` to match Scala in 2.0.\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  checkpoint(self, eager=True)\n",
      " |      Returns a checkpointed version of this Dataset. Checkpointing can be used to truncate the\n",
      " |      logical plan of this :class:`DataFrame`, which is especially useful in iterative algorithms\n",
      " |      where the plan may grow exponentially. It will be saved to files inside the checkpoint\n",
      " |      directory set with :meth:`SparkContext.setCheckpointDir`.\n",
      " |      \n",
      " |      :param eager: Whether to checkpoint this :class:`DataFrame` immediately\n",
      " |      \n",
      " |      .. note:: Experimental\n",
      " |      \n",
      " |      .. versionadded:: 2.1\n",
      " |  \n",
      " |  coalesce(self, numPartitions)\n",
      " |      Returns a new :class:`DataFrame` that has exactly `numPartitions` partitions.\n",
      " |      \n",
      " |      :param numPartitions: int, to specify the target number of partitions\n",
      " |      \n",
      " |      Similar to coalesce defined on an :class:`RDD`, this operation results in a\n",
      " |      narrow dependency, e.g. if you go from 1000 partitions to 100 partitions,\n",
      " |      there will not be a shuffle, instead each of the 100 new partitions will\n",
      " |      claim 10 of the current partitions. If a larger number of partitions is requested,\n",
      " |      it will stay at the current number of partitions.\n",
      " |      \n",
      " |      However, if you're doing a drastic coalesce, e.g. to numPartitions = 1,\n",
      " |      this may result in your computation taking place on fewer nodes than\n",
      " |      you like (e.g. one node in the case of numPartitions = 1). To avoid this,\n",
      " |      you can call repartition(). This will add a shuffle step, but means the\n",
      " |      current upstream partitions will be executed in parallel (per whatever\n",
      " |      the current partitioning is).\n",
      " |      \n",
      " |      >>> df.coalesce(1).rdd.getNumPartitions()\n",
      " |      1\n",
      " |      \n",
      " |      .. versionadded:: 1.4\n",
      " |  \n",
      " |  colRegex(self, colName)\n",
      " |      Selects column based on the column name specified as a regex and returns it\n",
      " |      as :class:`Column`.\n",
      " |      \n",
      " |      :param colName: string, column name specified as a regex.\n",
      " |      \n",
      " |      >>> df = spark.createDataFrame([(\"a\", 1), (\"b\", 2), (\"c\",  3)], [\"Col1\", \"Col2\"])\n",
      " |      >>> df.select(df.colRegex(\"`(Col1)?+.+`\")).show()\n",
      " |      +----+\n",
      " |      |Col2|\n",
      " |      +----+\n",
      " |      |   1|\n",
      " |      |   2|\n",
      " |      |   3|\n",
      " |      +----+\n",
      " |      \n",
      " |      .. versionadded:: 2.3\n",
      " |  \n",
      " |  collect(self)\n",
      " |      Returns all the records as a list of :class:`Row`.\n",
      " |      \n",
      " |      >>> df.collect()\n",
      " |      [Row(age=2, name='Alice'), Row(age=5, name='Bob')]\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  corr(self, col1, col2, method=None)\n",
      " |      Calculates the correlation of two columns of a :class:`DataFrame` as a double value.\n",
      " |      Currently only supports the Pearson Correlation Coefficient.\n",
      " |      :func:`DataFrame.corr` and :func:`DataFrameStatFunctions.corr` are aliases of each other.\n",
      " |      \n",
      " |      :param col1: The name of the first column\n",
      " |      :param col2: The name of the second column\n",
      " |      :param method: The correlation method. Currently only supports \"pearson\"\n",
      " |      \n",
      " |      .. versionadded:: 1.4\n",
      " |  \n",
      " |  count(self)\n",
      " |      Returns the number of rows in this :class:`DataFrame`.\n",
      " |      \n",
      " |      >>> df.count()\n",
      " |      2\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  cov(self, col1, col2)\n",
      " |      Calculate the sample covariance for the given columns, specified by their names, as a\n",
      " |      double value. :func:`DataFrame.cov` and :func:`DataFrameStatFunctions.cov` are aliases.\n",
      " |      \n",
      " |      :param col1: The name of the first column\n",
      " |      :param col2: The name of the second column\n",
      " |      \n",
      " |      .. versionadded:: 1.4\n",
      " |  \n",
      " |  createGlobalTempView(self, name)\n",
      " |      Creates a global temporary view with this :class:`DataFrame`.\n",
      " |      \n",
      " |      The lifetime of this temporary view is tied to this Spark application.\n",
      " |      throws :class:`TempTableAlreadyExistsException`, if the view name already exists in the\n",
      " |      catalog.\n",
      " |      \n",
      " |      >>> df.createGlobalTempView(\"people\")\n",
      " |      >>> df2 = spark.sql(\"select * from global_temp.people\")\n",
      " |      >>> sorted(df.collect()) == sorted(df2.collect())\n",
      " |      True\n",
      " |      >>> df.createGlobalTempView(\"people\")  # doctest: +IGNORE_EXCEPTION_DETAIL\n",
      " |      Traceback (most recent call last):\n",
      " |      ...\n",
      " |      AnalysisException: u\"Temporary table 'people' already exists;\"\n",
      " |      >>> spark.catalog.dropGlobalTempView(\"people\")\n",
      " |      \n",
      " |      .. versionadded:: 2.1\n",
      " |  \n",
      " |  createOrReplaceGlobalTempView(self, name)\n",
      " |      Creates or replaces a global temporary view using the given name.\n",
      " |      \n",
      " |      The lifetime of this temporary view is tied to this Spark application.\n",
      " |      \n",
      " |      >>> df.createOrReplaceGlobalTempView(\"people\")\n",
      " |      >>> df2 = df.filter(df.age > 3)\n",
      " |      >>> df2.createOrReplaceGlobalTempView(\"people\")\n",
      " |      >>> df3 = spark.sql(\"select * from global_temp.people\")\n",
      " |      >>> sorted(df3.collect()) == sorted(df2.collect())\n",
      " |      True\n",
      " |      >>> spark.catalog.dropGlobalTempView(\"people\")\n",
      " |      \n",
      " |      .. versionadded:: 2.2\n",
      " |  \n",
      " |  createOrReplaceTempView(self, name)\n",
      " |      Creates or replaces a local temporary view with this :class:`DataFrame`.\n",
      " |      \n",
      " |      The lifetime of this temporary table is tied to the :class:`SparkSession`\n",
      " |      that was used to create this :class:`DataFrame`.\n",
      " |      \n",
      " |      >>> df.createOrReplaceTempView(\"people\")\n",
      " |      >>> df2 = df.filter(df.age > 3)\n",
      " |      >>> df2.createOrReplaceTempView(\"people\")\n",
      " |      >>> df3 = spark.sql(\"select * from people\")\n",
      " |      >>> sorted(df3.collect()) == sorted(df2.collect())\n",
      " |      True\n",
      " |      >>> spark.catalog.dropTempView(\"people\")\n",
      " |      \n",
      " |      .. versionadded:: 2.0\n",
      " |  \n",
      " |  createTempView(self, name)\n",
      " |      Creates a local temporary view with this :class:`DataFrame`.\n",
      " |      \n",
      " |      The lifetime of this temporary table is tied to the :class:`SparkSession`\n",
      " |      that was used to create this :class:`DataFrame`.\n",
      " |      throws :class:`TempTableAlreadyExistsException`, if the view name already exists in the\n",
      " |      catalog.\n",
      " |      \n",
      " |      >>> df.createTempView(\"people\")\n",
      " |      >>> df2 = spark.sql(\"select * from people\")\n",
      " |      >>> sorted(df.collect()) == sorted(df2.collect())\n",
      " |      True\n",
      " |      >>> df.createTempView(\"people\")  # doctest: +IGNORE_EXCEPTION_DETAIL\n",
      " |      Traceback (most recent call last):\n",
      " |      ...\n",
      " |      AnalysisException: u\"Temporary table 'people' already exists;\"\n",
      " |      >>> spark.catalog.dropTempView(\"people\")\n",
      " |      \n",
      " |      .. versionadded:: 2.0\n",
      " |  \n",
      " |  crossJoin(self, other)\n",
      " |      Returns the cartesian product with another :class:`DataFrame`.\n",
      " |      \n",
      " |      :param other: Right side of the cartesian product.\n",
      " |      \n",
      " |      >>> df.select(\"age\", \"name\").collect()\n",
      " |      [Row(age=2, name='Alice'), Row(age=5, name='Bob')]\n",
      " |      >>> df2.select(\"name\", \"height\").collect()\n",
      " |      [Row(name='Tom', height=80), Row(name='Bob', height=85)]\n",
      " |      >>> df.crossJoin(df2.select(\"height\")).select(\"age\", \"name\", \"height\").collect()\n",
      " |      [Row(age=2, name='Alice', height=80), Row(age=2, name='Alice', height=85),\n",
      " |       Row(age=5, name='Bob', height=80), Row(age=5, name='Bob', height=85)]\n",
      " |      \n",
      " |      .. versionadded:: 2.1\n",
      " |  \n",
      " |  crosstab(self, col1, col2)\n",
      " |      Computes a pair-wise frequency table of the given columns. Also known as a contingency\n",
      " |      table. The number of distinct values for each column should be less than 1e4. At most 1e6\n",
      " |      non-zero pair frequencies will be returned.\n",
      " |      The first column of each row will be the distinct values of `col1` and the column names\n",
      " |      will be the distinct values of `col2`. The name of the first column will be `$col1_$col2`.\n",
      " |      Pairs that have no occurrences will have zero as their counts.\n",
      " |      :func:`DataFrame.crosstab` and :func:`DataFrameStatFunctions.crosstab` are aliases.\n",
      " |      \n",
      " |      :param col1: The name of the first column. Distinct items will make the first item of\n",
      " |          each row.\n",
      " |      :param col2: The name of the second column. Distinct items will make the column names\n",
      " |          of the :class:`DataFrame`.\n",
      " |      \n",
      " |      .. versionadded:: 1.4\n",
      " |  \n",
      " |  cube(self, *cols)\n",
      " |      Create a multi-dimensional cube for the current :class:`DataFrame` using\n",
      " |      the specified columns, so we can run aggregations on them.\n",
      " |      \n",
      " |      >>> df.cube(\"name\", df.age).count().orderBy(\"name\", \"age\").show()\n",
      " |      +-----+----+-----+\n",
      " |      | name| age|count|\n",
      " |      +-----+----+-----+\n",
      " |      | null|null|    2|\n",
      " |      | null|   2|    1|\n",
      " |      | null|   5|    1|\n",
      " |      |Alice|null|    1|\n",
      " |      |Alice|   2|    1|\n",
      " |      |  Bob|null|    1|\n",
      " |      |  Bob|   5|    1|\n",
      " |      +-----+----+-----+\n",
      " |      \n",
      " |      .. versionadded:: 1.4\n",
      " |  \n",
      " |  describe(self, *cols)\n",
      " |      Computes basic statistics for numeric and string columns.\n",
      " |      \n",
      " |      This include count, mean, stddev, min, and max. If no columns are\n",
      " |      given, this function computes statistics for all numerical or string columns.\n",
      " |      \n",
      " |      .. note:: This function is meant for exploratory data analysis, as we make no\n",
      " |          guarantee about the backward compatibility of the schema of the resulting\n",
      " |          :class:`DataFrame`.\n",
      " |      \n",
      " |      >>> df.describe(['age']).show()\n",
      " |      +-------+------------------+\n",
      " |      |summary|               age|\n",
      " |      +-------+------------------+\n",
      " |      |  count|                 2|\n",
      " |      |   mean|               3.5|\n",
      " |      | stddev|2.1213203435596424|\n",
      " |      |    min|                 2|\n",
      " |      |    max|                 5|\n",
      " |      +-------+------------------+\n",
      " |      >>> df.describe().show()\n",
      " |      +-------+------------------+-----+\n",
      " |      |summary|               age| name|\n",
      " |      +-------+------------------+-----+\n",
      " |      |  count|                 2|    2|\n",
      " |      |   mean|               3.5| null|\n",
      " |      | stddev|2.1213203435596424| null|\n",
      " |      |    min|                 2|Alice|\n",
      " |      |    max|                 5|  Bob|\n",
      " |      +-------+------------------+-----+\n",
      " |      \n",
      " |      Use summary for expanded statistics and control over which statistics to compute.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.1\n",
      " |  \n",
      " |  distinct(self)\n",
      " |      Returns a new :class:`DataFrame` containing the distinct rows in this :class:`DataFrame`.\n",
      " |      \n",
      " |      >>> df.distinct().count()\n",
      " |      2\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  drop(self, *cols)\n",
      " |      Returns a new :class:`DataFrame` that drops the specified column.\n",
      " |      This is a no-op if schema doesn't contain the given column name(s).\n",
      " |      \n",
      " |      :param cols: a string name of the column to drop, or a\n",
      " |          :class:`Column` to drop, or a list of string name of the columns to drop.\n",
      " |      \n",
      " |      >>> df.drop('age').collect()\n",
      " |      [Row(name='Alice'), Row(name='Bob')]\n",
      " |      \n",
      " |      >>> df.drop(df.age).collect()\n",
      " |      [Row(name='Alice'), Row(name='Bob')]\n",
      " |      \n",
      " |      >>> df.join(df2, df.name == df2.name, 'inner').drop(df.name).collect()\n",
      " |      [Row(age=5, height=85, name='Bob')]\n",
      " |      \n",
      " |      >>> df.join(df2, df.name == df2.name, 'inner').drop(df2.name).collect()\n",
      " |      [Row(age=5, name='Bob', height=85)]\n",
      " |      \n",
      " |      >>> df.join(df2, 'name', 'inner').drop('age', 'height').collect()\n",
      " |      [Row(name='Bob')]\n",
      " |      \n",
      " |      .. versionadded:: 1.4\n",
      " |  \n",
      " |  dropDuplicates(self, subset=None)\n",
      " |      Return a new :class:`DataFrame` with duplicate rows removed,\n",
      " |      optionally only considering certain columns.\n",
      " |      \n",
      " |      For a static batch :class:`DataFrame`, it just drops duplicate rows. For a streaming\n",
      " |      :class:`DataFrame`, it will keep all data across triggers as intermediate state to drop\n",
      " |      duplicates rows. You can use :func:`withWatermark` to limit how late the duplicate data can\n",
      " |      be and system will accordingly limit the state. In addition, too late data older than\n",
      " |      watermark will be dropped to avoid any possibility of duplicates.\n",
      " |      \n",
      " |      :func:`drop_duplicates` is an alias for :func:`dropDuplicates`.\n",
      " |      \n",
      " |      >>> from pyspark.sql import Row\n",
      " |      >>> df = sc.parallelize([ \\\n",
      " |      ...     Row(name='Alice', age=5, height=80), \\\n",
      " |      ...     Row(name='Alice', age=5, height=80), \\\n",
      " |      ...     Row(name='Alice', age=10, height=80)]).toDF()\n",
      " |      >>> df.dropDuplicates().show()\n",
      " |      +---+------+-----+\n",
      " |      |age|height| name|\n",
      " |      +---+------+-----+\n",
      " |      |  5|    80|Alice|\n",
      " |      | 10|    80|Alice|\n",
      " |      +---+------+-----+\n",
      " |      \n",
      " |      >>> df.dropDuplicates(['name', 'height']).show()\n",
      " |      +---+------+-----+\n",
      " |      |age|height| name|\n",
      " |      +---+------+-----+\n",
      " |      |  5|    80|Alice|\n",
      " |      +---+------+-----+\n",
      " |      \n",
      " |      .. versionadded:: 1.4\n",
      " |  \n",
      " |  drop_duplicates = dropDuplicates(self, subset=None)\n",
      " |      :func:`drop_duplicates` is an alias for :func:`dropDuplicates`.\n",
      " |      \n",
      " |      .. versionadded:: 1.4\n",
      " |  \n",
      " |  dropna(self, how='any', thresh=None, subset=None)\n",
      " |      Returns a new :class:`DataFrame` omitting rows with null values.\n",
      " |      :func:`DataFrame.dropna` and :func:`DataFrameNaFunctions.drop` are aliases of each other.\n",
      " |      \n",
      " |      :param how: 'any' or 'all'.\n",
      " |          If 'any', drop a row if it contains any nulls.\n",
      " |          If 'all', drop a row only if all its values are null.\n",
      " |      :param thresh: int, default None\n",
      " |          If specified, drop rows that have less than `thresh` non-null values.\n",
      " |          This overwrites the `how` parameter.\n",
      " |      :param subset: optional list of column names to consider.\n",
      " |      \n",
      " |      >>> df4.na.drop().show()\n",
      " |      +---+------+-----+\n",
      " |      |age|height| name|\n",
      " |      +---+------+-----+\n",
      " |      | 10|    80|Alice|\n",
      " |      +---+------+-----+\n",
      " |      \n",
      " |      .. versionadded:: 1.3.1\n",
      " |  \n",
      " |  exceptAll(self, other)\n",
      " |      Return a new :class:`DataFrame` containing rows in this :class:`DataFrame` but\n",
      " |      not in another :class:`DataFrame` while preserving duplicates.\n",
      " |      \n",
      " |      This is equivalent to `EXCEPT ALL` in SQL.\n",
      " |      \n",
      " |      >>> df1 = spark.createDataFrame(\n",
      " |      ...         [(\"a\", 1), (\"a\", 1), (\"a\", 1), (\"a\", 2), (\"b\",  3), (\"c\", 4)], [\"C1\", \"C2\"])\n",
      " |      >>> df2 = spark.createDataFrame([(\"a\", 1), (\"b\", 3)], [\"C1\", \"C2\"])\n",
      " |      \n",
      " |      >>> df1.exceptAll(df2).show()\n",
      " |      +---+---+\n",
      " |      | C1| C2|\n",
      " |      +---+---+\n",
      " |      |  a|  1|\n",
      " |      |  a|  1|\n",
      " |      |  a|  2|\n",
      " |      |  c|  4|\n",
      " |      +---+---+\n",
      " |      \n",
      " |      Also as standard in SQL, this function resolves columns by position (not by name).\n",
      " |      \n",
      " |      .. versionadded:: 2.4\n",
      " |  \n",
      " |  explain(self, extended=None, mode=None)\n",
      " |      Prints the (logical and physical) plans to the console for debugging purpose.\n",
      " |      \n",
      " |      :param extended: boolean, default ``False``. If ``False``, prints only the physical plan.\n",
      " |          When this is a string without specifying the ``mode``, it works as the mode is\n",
      " |          specified.\n",
      " |      :param mode: specifies the expected output format of plans.\n",
      " |      \n",
      " |          * ``simple``: Print only a physical plan.\n",
      " |          * ``extended``: Print both logical and physical plans.\n",
      " |          * ``codegen``: Print a physical plan and generated codes if they are available.\n",
      " |          * ``cost``: Print a logical plan and statistics if they are available.\n",
      " |          * ``formatted``: Split explain output into two sections: a physical plan outline                 and node details.\n",
      " |      \n",
      " |      >>> df.explain()\n",
      " |      == Physical Plan ==\n",
      " |      *(1) Scan ExistingRDD[age#0,name#1]\n",
      " |      \n",
      " |      >>> df.explain(True)\n",
      " |      == Parsed Logical Plan ==\n",
      " |      ...\n",
      " |      == Analyzed Logical Plan ==\n",
      " |      ...\n",
      " |      == Optimized Logical Plan ==\n",
      " |      ...\n",
      " |      == Physical Plan ==\n",
      " |      ...\n",
      " |      \n",
      " |      >>> df.explain(mode=\"formatted\")\n",
      " |      == Physical Plan ==\n",
      " |      * Scan ExistingRDD (1)\n",
      " |      (1) Scan ExistingRDD [codegen id : 1]\n",
      " |      Output [2]: [age#0, name#1]\n",
      " |      ...\n",
      " |      \n",
      " |      >>> df.explain(\"cost\")\n",
      " |      == Optimized Logical Plan ==\n",
      " |      ...Statistics...\n",
      " |      ...\n",
      " |      \n",
      " |      .. versionchanged:: 3.0.0\n",
      " |         Added optional argument `mode` to specify the expected output format of plans.\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  fillna(self, value, subset=None)\n",
      " |      Replace null values, alias for ``na.fill()``.\n",
      " |      :func:`DataFrame.fillna` and :func:`DataFrameNaFunctions.fill` are aliases of each other.\n",
      " |      \n",
      " |      :param value: int, long, float, string, bool or dict.\n",
      " |          Value to replace null values with.\n",
      " |          If the value is a dict, then `subset` is ignored and `value` must be a mapping\n",
      " |          from column name (string) to replacement value. The replacement value must be\n",
      " |          an int, long, float, boolean, or string.\n",
      " |      :param subset: optional list of column names to consider.\n",
      " |          Columns specified in subset that do not have matching data type are ignored.\n",
      " |          For example, if `value` is a string, and subset contains a non-string column,\n",
      " |          then the non-string column is simply ignored.\n",
      " |      \n",
      " |      >>> df4.na.fill(50).show()\n",
      " |      +---+------+-----+\n",
      " |      |age|height| name|\n",
      " |      +---+------+-----+\n",
      " |      | 10|    80|Alice|\n",
      " |      |  5|    50|  Bob|\n",
      " |      | 50|    50|  Tom|\n",
      " |      | 50|    50| null|\n",
      " |      +---+------+-----+\n",
      " |      \n",
      " |      >>> df5.na.fill(False).show()\n",
      " |      +----+-------+-----+\n",
      " |      | age|   name|  spy|\n",
      " |      +----+-------+-----+\n",
      " |      |  10|  Alice|false|\n",
      " |      |   5|    Bob|false|\n",
      " |      |null|Mallory| true|\n",
      " |      +----+-------+-----+\n",
      " |      \n",
      " |      >>> df4.na.fill({'age': 50, 'name': 'unknown'}).show()\n",
      " |      +---+------+-------+\n",
      " |      |age|height|   name|\n",
      " |      +---+------+-------+\n",
      " |      | 10|    80|  Alice|\n",
      " |      |  5|  null|    Bob|\n",
      " |      | 50|  null|    Tom|\n",
      " |      | 50|  null|unknown|\n",
      " |      +---+------+-------+\n",
      " |      \n",
      " |      .. versionadded:: 1.3.1\n",
      " |  \n",
      " |  filter(self, condition)\n",
      " |      Filters rows using the given condition.\n",
      " |      \n",
      " |      :func:`where` is an alias for :func:`filter`.\n",
      " |      \n",
      " |      :param condition: a :class:`Column` of :class:`types.BooleanType`\n",
      " |          or a string of SQL expression.\n",
      " |      \n",
      " |      >>> df.filter(df.age > 3).collect()\n",
      " |      [Row(age=5, name='Bob')]\n",
      " |      >>> df.where(df.age == 2).collect()\n",
      " |      [Row(age=2, name='Alice')]\n",
      " |      \n",
      " |      >>> df.filter(\"age > 3\").collect()\n",
      " |      [Row(age=5, name='Bob')]\n",
      " |      >>> df.where(\"age = 2\").collect()\n",
      " |      [Row(age=2, name='Alice')]\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  first(self)\n",
      " |      Returns the first row as a :class:`Row`.\n",
      " |      \n",
      " |      >>> df.first()\n",
      " |      Row(age=2, name='Alice')\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  foreach(self, f)\n",
      " |      Applies the ``f`` function to all :class:`Row` of this :class:`DataFrame`.\n",
      " |      \n",
      " |      This is a shorthand for ``df.rdd.foreach()``.\n",
      " |      \n",
      " |      >>> def f(person):\n",
      " |      ...     print(person.name)\n",
      " |      >>> df.foreach(f)\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  foreachPartition(self, f)\n",
      " |      Applies the ``f`` function to each partition of this :class:`DataFrame`.\n",
      " |      \n",
      " |      This a shorthand for ``df.rdd.foreachPartition()``.\n",
      " |      \n",
      " |      >>> def f(people):\n",
      " |      ...     for person in people:\n",
      " |      ...         print(person.name)\n",
      " |      >>> df.foreachPartition(f)\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  freqItems(self, cols, support=None)\n",
      " |      Finding frequent items for columns, possibly with false positives. Using the\n",
      " |      frequent element count algorithm described in\n",
      " |      \"https://doi.org/10.1145/762471.762473, proposed by Karp, Schenker, and Papadimitriou\".\n",
      " |      :func:`DataFrame.freqItems` and :func:`DataFrameStatFunctions.freqItems` are aliases.\n",
      " |      \n",
      " |      .. note:: This function is meant for exploratory data analysis, as we make no\n",
      " |          guarantee about the backward compatibility of the schema of the resulting\n",
      " |          :class:`DataFrame`.\n",
      " |      \n",
      " |      :param cols: Names of the columns to calculate frequent items for as a list or tuple of\n",
      " |          strings.\n",
      " |      :param support: The frequency with which to consider an item 'frequent'. Default is 1%.\n",
      " |          The support must be greater than 1e-4.\n",
      " |      \n",
      " |      .. versionadded:: 1.4\n",
      " |  \n",
      " |  groupBy(self, *cols)\n",
      " |      Groups the :class:`DataFrame` using the specified columns,\n",
      " |      so we can run aggregation on them. See :class:`GroupedData`\n",
      " |      for all the available aggregate functions.\n",
      " |      \n",
      " |      :func:`groupby` is an alias for :func:`groupBy`.\n",
      " |      \n",
      " |      :param cols: list of columns to group by.\n",
      " |          Each element should be a column name (string) or an expression (:class:`Column`).\n",
      " |      \n",
      " |      >>> df.groupBy().avg().collect()\n",
      " |      [Row(avg(age)=3.5)]\n",
      " |      >>> sorted(df.groupBy('name').agg({'age': 'mean'}).collect())\n",
      " |      [Row(name='Alice', avg(age)=2.0), Row(name='Bob', avg(age)=5.0)]\n",
      " |      >>> sorted(df.groupBy(df.name).avg().collect())\n",
      " |      [Row(name='Alice', avg(age)=2.0), Row(name='Bob', avg(age)=5.0)]\n",
      " |      >>> sorted(df.groupBy(['name', df.age]).count().collect())\n",
      " |      [Row(name='Alice', age=2, count=1), Row(name='Bob', age=5, count=1)]\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  groupby = groupBy(self, *cols)\n",
      " |      :func:`groupby` is an alias for :func:`groupBy`.\n",
      " |      \n",
      " |      .. versionadded:: 1.4\n",
      " |  \n",
      " |  head(self, n=None)\n",
      " |      Returns the first ``n`` rows.\n",
      " |      \n",
      " |      .. note:: This method should only be used if the resulting array is expected\n",
      " |          to be small, as all the data is loaded into the driver's memory.\n",
      " |      \n",
      " |      :param n: int, default 1. Number of rows to return.\n",
      " |      :return: If n is greater than 1, return a list of :class:`Row`.\n",
      " |          If n is 1, return a single Row.\n",
      " |      \n",
      " |      >>> df.head()\n",
      " |      Row(age=2, name='Alice')\n",
      " |      >>> df.head(1)\n",
      " |      [Row(age=2, name='Alice')]\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  hint(self, name, *parameters)\n",
      " |      Specifies some hint on the current :class:`DataFrame`.\n",
      " |      \n",
      " |      :param name: A name of the hint.\n",
      " |      :param parameters: Optional parameters.\n",
      " |      :return: :class:`DataFrame`\n",
      " |      \n",
      " |      >>> df.join(df2.hint(\"broadcast\"), \"name\").show()\n",
      " |      +----+---+------+\n",
      " |      |name|age|height|\n",
      " |      +----+---+------+\n",
      " |      | Bob|  5|    85|\n",
      " |      +----+---+------+\n",
      " |      \n",
      " |      .. versionadded:: 2.2\n",
      " |  \n",
      " |  intersect(self, other)\n",
      " |      Return a new :class:`DataFrame` containing rows only in\n",
      " |      both this :class:`DataFrame` and another :class:`DataFrame`.\n",
      " |      \n",
      " |      This is equivalent to `INTERSECT` in SQL.\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  intersectAll(self, other)\n",
      " |      Return a new :class:`DataFrame` containing rows in both this :class:`DataFrame`\n",
      " |      and another :class:`DataFrame` while preserving duplicates.\n",
      " |      \n",
      " |      This is equivalent to `INTERSECT ALL` in SQL.\n",
      " |      >>> df1 = spark.createDataFrame([(\"a\", 1), (\"a\", 1), (\"b\", 3), (\"c\", 4)], [\"C1\", \"C2\"])\n",
      " |      >>> df2 = spark.createDataFrame([(\"a\", 1), (\"a\", 1), (\"b\", 3)], [\"C1\", \"C2\"])\n",
      " |      \n",
      " |      >>> df1.intersectAll(df2).sort(\"C1\", \"C2\").show()\n",
      " |      +---+---+\n",
      " |      | C1| C2|\n",
      " |      +---+---+\n",
      " |      |  a|  1|\n",
      " |      |  a|  1|\n",
      " |      |  b|  3|\n",
      " |      +---+---+\n",
      " |      \n",
      " |      Also as standard in SQL, this function resolves columns by position (not by name).\n",
      " |      \n",
      " |      .. versionadded:: 2.4\n",
      " |  \n",
      " |  isLocal(self)\n",
      " |      Returns ``True`` if the :func:`collect` and :func:`take` methods can be run locally\n",
      " |      (without any Spark executors).\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  join(self, other, on=None, how=None)\n",
      " |      Joins with another :class:`DataFrame`, using the given join expression.\n",
      " |      \n",
      " |      :param other: Right side of the join\n",
      " |      :param on: a string for the join column name, a list of column names,\n",
      " |          a join expression (Column), or a list of Columns.\n",
      " |          If `on` is a string or a list of strings indicating the name of the join column(s),\n",
      " |          the column(s) must exist on both sides, and this performs an equi-join.\n",
      " |      :param how: str, default ``inner``. Must be one of: ``inner``, ``cross``, ``outer``,\n",
      " |          ``full``, ``fullouter``, ``full_outer``, ``left``, ``leftouter``, ``left_outer``,\n",
      " |          ``right``, ``rightouter``, ``right_outer``, ``semi``, ``leftsemi``, ``left_semi``,\n",
      " |          ``anti``, ``leftanti`` and ``left_anti``.\n",
      " |      \n",
      " |      The following performs a full outer join between ``df1`` and ``df2``.\n",
      " |      >>> from pyspark.sql.functions import desc\n",
      " |      >>> df.join(df2, df.name == df2.name, 'outer').select(df.name, df2.height)                 .sort(desc(\"name\")).collect()\n",
      " |      [Row(name='Bob', height=85), Row(name='Alice', height=None), Row(name=None, height=80)]\n",
      " |      \n",
      " |      >>> df.join(df2, 'name', 'outer').select('name', 'height').sort(desc(\"name\")).collect()\n",
      " |      [Row(name='Tom', height=80), Row(name='Bob', height=85), Row(name='Alice', height=None)]\n",
      " |      \n",
      " |      >>> cond = [df.name == df3.name, df.age == df3.age]\n",
      " |      >>> df.join(df3, cond, 'outer').select(df.name, df3.age).collect()\n",
      " |      [Row(name='Alice', age=2), Row(name='Bob', age=5)]\n",
      " |      \n",
      " |      >>> df.join(df2, 'name').select(df.name, df2.height).collect()\n",
      " |      [Row(name='Bob', height=85)]\n",
      " |      \n",
      " |      >>> df.join(df4, ['name', 'age']).select(df.name, df.age).collect()\n",
      " |      [Row(name='Bob', age=5)]\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  limit(self, num)\n",
      " |      Limits the result count to the number specified.\n",
      " |      \n",
      " |      >>> df.limit(1).collect()\n",
      " |      [Row(age=2, name='Alice')]\n",
      " |      >>> df.limit(0).collect()\n",
      " |      []\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  localCheckpoint(self, eager=True)\n",
      " |      Returns a locally checkpointed version of this Dataset. Checkpointing can be used to\n",
      " |      truncate the logical plan of this :class:`DataFrame`, which is especially useful in\n",
      " |      iterative algorithms where the plan may grow exponentially. Local checkpoints are\n",
      " |      stored in the executors using the caching subsystem and therefore they are not reliable.\n",
      " |      \n",
      " |      :param eager: Whether to checkpoint this :class:`DataFrame` immediately\n",
      " |      \n",
      " |      .. note:: Experimental\n",
      " |      \n",
      " |      .. versionadded:: 2.3\n",
      " |  \n",
      " |  orderBy = sort(self, *cols, **kwargs)\n",
      " |  \n",
      " |  persist(self, storageLevel=StorageLevel(True, True, False, False, 1))\n",
      " |      Sets the storage level to persist the contents of the :class:`DataFrame` across\n",
      " |      operations after the first time it is computed. This can only be used to assign\n",
      " |      a new storage level if the :class:`DataFrame` does not have a storage level set yet.\n",
      " |      If no storage level is specified defaults to (`MEMORY_AND_DISK`).\n",
      " |      \n",
      " |      .. note:: The default storage level has changed to `MEMORY_AND_DISK` to match Scala in 2.0.\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  printSchema(self)\n",
      " |      Prints out the schema in the tree format.\n",
      " |      \n",
      " |      >>> df.printSchema()\n",
      " |      root\n",
      " |       |-- age: integer (nullable = true)\n",
      " |       |-- name: string (nullable = true)\n",
      " |      <BLANKLINE>\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  randomSplit(self, weights, seed=None)\n",
      " |      Randomly splits this :class:`DataFrame` with the provided weights.\n",
      " |      \n",
      " |      :param weights: list of doubles as weights with which to split the :class:`DataFrame`.\n",
      " |          Weights will be normalized if they don't sum up to 1.0.\n",
      " |      :param seed: The seed for sampling.\n",
      " |      \n",
      " |      >>> splits = df4.randomSplit([1.0, 2.0], 24)\n",
      " |      >>> splits[0].count()\n",
      " |      2\n",
      " |      \n",
      " |      >>> splits[1].count()\n",
      " |      2\n",
      " |      \n",
      " |      .. versionadded:: 1.4\n",
      " |  \n",
      " |  registerTempTable(self, name)\n",
      " |      Registers this DataFrame as a temporary table using the given name.\n",
      " |      \n",
      " |      The lifetime of this temporary table is tied to the :class:`SparkSession`\n",
      " |      that was used to create this :class:`DataFrame`.\n",
      " |      \n",
      " |      >>> df.registerTempTable(\"people\")\n",
      " |      >>> df2 = spark.sql(\"select * from people\")\n",
      " |      >>> sorted(df.collect()) == sorted(df2.collect())\n",
      " |      True\n",
      " |      >>> spark.catalog.dropTempView(\"people\")\n",
      " |      \n",
      " |      .. note:: Deprecated in 2.0, use createOrReplaceTempView instead.\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  repartition(self, numPartitions, *cols)\n",
      " |      Returns a new :class:`DataFrame` partitioned by the given partitioning expressions. The\n",
      " |      resulting :class:`DataFrame` is hash partitioned.\n",
      " |      \n",
      " |      :param numPartitions:\n",
      " |          can be an int to specify the target number of partitions or a Column.\n",
      " |          If it is a Column, it will be used as the first partitioning column. If not specified,\n",
      " |          the default number of partitions is used.\n",
      " |      \n",
      " |      .. versionchanged:: 1.6\n",
      " |         Added optional arguments to specify the partitioning columns. Also made numPartitions\n",
      " |         optional if partitioning columns are specified.\n",
      " |      \n",
      " |      >>> df.repartition(10).rdd.getNumPartitions()\n",
      " |      10\n",
      " |      >>> data = df.union(df).repartition(\"age\")\n",
      " |      >>> data.show()\n",
      " |      +---+-----+\n",
      " |      |age| name|\n",
      " |      +---+-----+\n",
      " |      |  5|  Bob|\n",
      " |      |  5|  Bob|\n",
      " |      |  2|Alice|\n",
      " |      |  2|Alice|\n",
      " |      +---+-----+\n",
      " |      >>> data = data.repartition(7, \"age\")\n",
      " |      >>> data.show()\n",
      " |      +---+-----+\n",
      " |      |age| name|\n",
      " |      +---+-----+\n",
      " |      |  2|Alice|\n",
      " |      |  5|  Bob|\n",
      " |      |  2|Alice|\n",
      " |      |  5|  Bob|\n",
      " |      +---+-----+\n",
      " |      >>> data.rdd.getNumPartitions()\n",
      " |      7\n",
      " |      >>> data = data.repartition(\"name\", \"age\")\n",
      " |      >>> data.show()\n",
      " |      +---+-----+\n",
      " |      |age| name|\n",
      " |      +---+-----+\n",
      " |      |  5|  Bob|\n",
      " |      |  5|  Bob|\n",
      " |      |  2|Alice|\n",
      " |      |  2|Alice|\n",
      " |      +---+-----+\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  repartitionByRange(self, numPartitions, *cols)\n",
      " |      Returns a new :class:`DataFrame` partitioned by the given partitioning expressions. The\n",
      " |      resulting :class:`DataFrame` is range partitioned.\n",
      " |      \n",
      " |      :param numPartitions:\n",
      " |          can be an int to specify the target number of partitions or a Column.\n",
      " |          If it is a Column, it will be used as the first partitioning column. If not specified,\n",
      " |          the default number of partitions is used.\n",
      " |      \n",
      " |      At least one partition-by expression must be specified.\n",
      " |      When no explicit sort order is specified, \"ascending nulls first\" is assumed.\n",
      " |      \n",
      " |      Note that due to performance reasons this method uses sampling to estimate the ranges.\n",
      " |      Hence, the output may not be consistent, since sampling can return different values.\n",
      " |      The sample size can be controlled by the config\n",
      " |      `spark.sql.execution.rangeExchange.sampleSizePerPartition`.\n",
      " |      \n",
      " |      >>> df.repartitionByRange(2, \"age\").rdd.getNumPartitions()\n",
      " |      2\n",
      " |      >>> df.show()\n",
      " |      +---+-----+\n",
      " |      |age| name|\n",
      " |      +---+-----+\n",
      " |      |  2|Alice|\n",
      " |      |  5|  Bob|\n",
      " |      +---+-----+\n",
      " |      >>> df.repartitionByRange(1, \"age\").rdd.getNumPartitions()\n",
      " |      1\n",
      " |      >>> data = df.repartitionByRange(\"age\")\n",
      " |      >>> df.show()\n",
      " |      +---+-----+\n",
      " |      |age| name|\n",
      " |      +---+-----+\n",
      " |      |  2|Alice|\n",
      " |      |  5|  Bob|\n",
      " |      +---+-----+\n",
      " |      \n",
      " |      .. versionadded:: 2.4.0\n",
      " |  \n",
      " |  replace(self, to_replace, value=<no value>, subset=None)\n",
      " |      Returns a new :class:`DataFrame` replacing a value with another value.\n",
      " |      :func:`DataFrame.replace` and :func:`DataFrameNaFunctions.replace` are\n",
      " |      aliases of each other.\n",
      " |      Values to_replace and value must have the same type and can only be numerics, booleans,\n",
      " |      or strings. Value can have None. When replacing, the new value will be cast\n",
      " |      to the type of the existing column.\n",
      " |      For numeric replacements all values to be replaced should have unique\n",
      " |      floating point representation. In case of conflicts (for example with `{42: -1, 42.0: 1}`)\n",
      " |      and arbitrary replacement will be used.\n",
      " |      \n",
      " |      :param to_replace: bool, int, long, float, string, list or dict.\n",
      " |          Value to be replaced.\n",
      " |          If the value is a dict, then `value` is ignored or can be omitted, and `to_replace`\n",
      " |          must be a mapping between a value and a replacement.\n",
      " |      :param value: bool, int, long, float, string, list or None.\n",
      " |          The replacement value must be a bool, int, long, float, string or None. If `value` is a\n",
      " |          list, `value` should be of the same length and type as `to_replace`.\n",
      " |          If `value` is a scalar and `to_replace` is a sequence, then `value` is\n",
      " |          used as a replacement for each item in `to_replace`.\n",
      " |      :param subset: optional list of column names to consider.\n",
      " |          Columns specified in subset that do not have matching data type are ignored.\n",
      " |          For example, if `value` is a string, and subset contains a non-string column,\n",
      " |          then the non-string column is simply ignored.\n",
      " |      \n",
      " |      >>> df4.na.replace(10, 20).show()\n",
      " |      +----+------+-----+\n",
      " |      | age|height| name|\n",
      " |      +----+------+-----+\n",
      " |      |  20|    80|Alice|\n",
      " |      |   5|  null|  Bob|\n",
      " |      |null|  null|  Tom|\n",
      " |      |null|  null| null|\n",
      " |      +----+------+-----+\n",
      " |      \n",
      " |      >>> df4.na.replace('Alice', None).show()\n",
      " |      +----+------+----+\n",
      " |      | age|height|name|\n",
      " |      +----+------+----+\n",
      " |      |  10|    80|null|\n",
      " |      |   5|  null| Bob|\n",
      " |      |null|  null| Tom|\n",
      " |      |null|  null|null|\n",
      " |      +----+------+----+\n",
      " |      \n",
      " |      >>> df4.na.replace({'Alice': None}).show()\n",
      " |      +----+------+----+\n",
      " |      | age|height|name|\n",
      " |      +----+------+----+\n",
      " |      |  10|    80|null|\n",
      " |      |   5|  null| Bob|\n",
      " |      |null|  null| Tom|\n",
      " |      |null|  null|null|\n",
      " |      +----+------+----+\n",
      " |      \n",
      " |      >>> df4.na.replace(['Alice', 'Bob'], ['A', 'B'], 'name').show()\n",
      " |      +----+------+----+\n",
      " |      | age|height|name|\n",
      " |      +----+------+----+\n",
      " |      |  10|    80|   A|\n",
      " |      |   5|  null|   B|\n",
      " |      |null|  null| Tom|\n",
      " |      |null|  null|null|\n",
      " |      +----+------+----+\n",
      " |      \n",
      " |      .. versionadded:: 1.4\n",
      " |  \n",
      " |  rollup(self, *cols)\n",
      " |      Create a multi-dimensional rollup for the current :class:`DataFrame` using\n",
      " |      the specified columns, so we can run aggregation on them.\n",
      " |      \n",
      " |      >>> df.rollup(\"name\", df.age).count().orderBy(\"name\", \"age\").show()\n",
      " |      +-----+----+-----+\n",
      " |      | name| age|count|\n",
      " |      +-----+----+-----+\n",
      " |      | null|null|    2|\n",
      " |      |Alice|null|    1|\n",
      " |      |Alice|   2|    1|\n",
      " |      |  Bob|null|    1|\n",
      " |      |  Bob|   5|    1|\n",
      " |      +-----+----+-----+\n",
      " |      \n",
      " |      .. versionadded:: 1.4\n",
      " |  \n",
      " |  sample(self, withReplacement=None, fraction=None, seed=None)\n",
      " |      Returns a sampled subset of this :class:`DataFrame`.\n",
      " |      \n",
      " |      :param withReplacement: Sample with replacement or not (default ``False``).\n",
      " |      :param fraction: Fraction of rows to generate, range [0.0, 1.0].\n",
      " |      :param seed: Seed for sampling (default a random seed).\n",
      " |      \n",
      " |      .. note:: This is not guaranteed to provide exactly the fraction specified of the total\n",
      " |          count of the given :class:`DataFrame`.\n",
      " |      \n",
      " |      .. note:: `fraction` is required and, `withReplacement` and `seed` are optional.\n",
      " |      \n",
      " |      >>> df = spark.range(10)\n",
      " |      >>> df.sample(0.5, 3).count()\n",
      " |      7\n",
      " |      >>> df.sample(fraction=0.5, seed=3).count()\n",
      " |      7\n",
      " |      >>> df.sample(withReplacement=True, fraction=0.5, seed=3).count()\n",
      " |      1\n",
      " |      >>> df.sample(1.0).count()\n",
      " |      10\n",
      " |      >>> df.sample(fraction=1.0).count()\n",
      " |      10\n",
      " |      >>> df.sample(False, fraction=1.0).count()\n",
      " |      10\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  sampleBy(self, col, fractions, seed=None)\n",
      " |      Returns a stratified sample without replacement based on the\n",
      " |      fraction given on each stratum.\n",
      " |      \n",
      " |      :param col: column that defines strata\n",
      " |      :param fractions:\n",
      " |          sampling fraction for each stratum. If a stratum is not\n",
      " |          specified, we treat its fraction as zero.\n",
      " |      :param seed: random seed\n",
      " |      :return: a new :class:`DataFrame` that represents the stratified sample\n",
      " |      \n",
      " |      >>> from pyspark.sql.functions import col\n",
      " |      >>> dataset = sqlContext.range(0, 100).select((col(\"id\") % 3).alias(\"key\"))\n",
      " |      >>> sampled = dataset.sampleBy(\"key\", fractions={0: 0.1, 1: 0.2}, seed=0)\n",
      " |      >>> sampled.groupBy(\"key\").count().orderBy(\"key\").show()\n",
      " |      +---+-----+\n",
      " |      |key|count|\n",
      " |      +---+-----+\n",
      " |      |  0|    3|\n",
      " |      |  1|    6|\n",
      " |      +---+-----+\n",
      " |      >>> dataset.sampleBy(col(\"key\"), fractions={2: 1.0}, seed=0).count()\n",
      " |      33\n",
      " |      \n",
      " |      .. versionchanged:: 3.0\n",
      " |         Added sampling by a column of :class:`Column`\n",
      " |      \n",
      " |      .. versionadded:: 1.5\n",
      " |  \n",
      " |  select(self, *cols)\n",
      " |      Projects a set of expressions and returns a new :class:`DataFrame`.\n",
      " |      \n",
      " |      :param cols: list of column names (string) or expressions (:class:`Column`).\n",
      " |          If one of the column names is '*', that column is expanded to include all columns\n",
      " |          in the current :class:`DataFrame`.\n",
      " |      \n",
      " |      >>> df.select('*').collect()\n",
      " |      [Row(age=2, name='Alice'), Row(age=5, name='Bob')]\n",
      " |      >>> df.select('name', 'age').collect()\n",
      " |      [Row(name='Alice', age=2), Row(name='Bob', age=5)]\n",
      " |      >>> df.select(df.name, (df.age + 10).alias('age')).collect()\n",
      " |      [Row(name='Alice', age=12), Row(name='Bob', age=15)]\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  selectExpr(self, *expr)\n",
      " |      Projects a set of SQL expressions and returns a new :class:`DataFrame`.\n",
      " |      \n",
      " |      This is a variant of :func:`select` that accepts SQL expressions.\n",
      " |      \n",
      " |      >>> df.selectExpr(\"age * 2\", \"abs(age)\").collect()\n",
      " |      [Row((age * 2)=4, abs(age)=2), Row((age * 2)=10, abs(age)=5)]\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  show(self, n=20, truncate=True, vertical=False)\n",
      " |      Prints the first ``n`` rows to the console.\n",
      " |      \n",
      " |      :param n: Number of rows to show.\n",
      " |      :param truncate: If set to ``True``, truncate strings longer than 20 chars by default.\n",
      " |          If set to a number greater than one, truncates long strings to length ``truncate``\n",
      " |          and align cells right.\n",
      " |      :param vertical: If set to ``True``, print output rows vertically (one line\n",
      " |          per column value).\n",
      " |      \n",
      " |      >>> df\n",
      " |      DataFrame[age: int, name: string]\n",
      " |      >>> df.show()\n",
      " |      +---+-----+\n",
      " |      |age| name|\n",
      " |      +---+-----+\n",
      " |      |  2|Alice|\n",
      " |      |  5|  Bob|\n",
      " |      +---+-----+\n",
      " |      >>> df.show(truncate=3)\n",
      " |      +---+----+\n",
      " |      |age|name|\n",
      " |      +---+----+\n",
      " |      |  2| Ali|\n",
      " |      |  5| Bob|\n",
      " |      +---+----+\n",
      " |      >>> df.show(vertical=True)\n",
      " |      -RECORD 0-----\n",
      " |       age  | 2\n",
      " |       name | Alice\n",
      " |      -RECORD 1-----\n",
      " |       age  | 5\n",
      " |       name | Bob\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  sort(self, *cols, **kwargs)\n",
      " |      Returns a new :class:`DataFrame` sorted by the specified column(s).\n",
      " |      \n",
      " |      :param cols: list of :class:`Column` or column names to sort by.\n",
      " |      :param ascending: boolean or list of boolean (default ``True``).\n",
      " |          Sort ascending vs. descending. Specify list for multiple sort orders.\n",
      " |          If a list is specified, length of the list must equal length of the `cols`.\n",
      " |      \n",
      " |      >>> df.sort(df.age.desc()).collect()\n",
      " |      [Row(age=5, name='Bob'), Row(age=2, name='Alice')]\n",
      " |      >>> df.sort(\"age\", ascending=False).collect()\n",
      " |      [Row(age=5, name='Bob'), Row(age=2, name='Alice')]\n",
      " |      >>> df.orderBy(df.age.desc()).collect()\n",
      " |      [Row(age=5, name='Bob'), Row(age=2, name='Alice')]\n",
      " |      >>> from pyspark.sql.functions import *\n",
      " |      >>> df.sort(asc(\"age\")).collect()\n",
      " |      [Row(age=2, name='Alice'), Row(age=5, name='Bob')]\n",
      " |      >>> df.orderBy(desc(\"age\"), \"name\").collect()\n",
      " |      [Row(age=5, name='Bob'), Row(age=2, name='Alice')]\n",
      " |      >>> df.orderBy([\"age\", \"name\"], ascending=[0, 1]).collect()\n",
      " |      [Row(age=5, name='Bob'), Row(age=2, name='Alice')]\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  sortWithinPartitions(self, *cols, **kwargs)\n",
      " |      Returns a new :class:`DataFrame` with each partition sorted by the specified column(s).\n",
      " |      \n",
      " |      :param cols: list of :class:`Column` or column names to sort by.\n",
      " |      :param ascending: boolean or list of boolean (default ``True``).\n",
      " |          Sort ascending vs. descending. Specify list for multiple sort orders.\n",
      " |          If a list is specified, length of the list must equal length of the `cols`.\n",
      " |      \n",
      " |      >>> df.sortWithinPartitions(\"age\", ascending=False).show()\n",
      " |      +---+-----+\n",
      " |      |age| name|\n",
      " |      +---+-----+\n",
      " |      |  2|Alice|\n",
      " |      |  5|  Bob|\n",
      " |      +---+-----+\n",
      " |      \n",
      " |      .. versionadded:: 1.6\n",
      " |  \n",
      " |  subtract(self, other)\n",
      " |      Return a new :class:`DataFrame` containing rows in this :class:`DataFrame`\n",
      " |      but not in another :class:`DataFrame`.\n",
      " |      \n",
      " |      This is equivalent to `EXCEPT DISTINCT` in SQL.\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  summary(self, *statistics)\n",
      " |      Computes specified statistics for numeric and string columns. Available statistics are:\n",
      " |      - count\n",
      " |      - mean\n",
      " |      - stddev\n",
      " |      - min\n",
      " |      - max\n",
      " |      - arbitrary approximate percentiles specified as a percentage (eg, 75%)\n",
      " |      \n",
      " |      If no statistics are given, this function computes count, mean, stddev, min,\n",
      " |      approximate quartiles (percentiles at 25%, 50%, and 75%), and max.\n",
      " |      \n",
      " |      .. note:: This function is meant for exploratory data analysis, as we make no\n",
      " |          guarantee about the backward compatibility of the schema of the resulting\n",
      " |          :class:`DataFrame`.\n",
      " |      \n",
      " |      >>> df.summary().show()\n",
      " |      +-------+------------------+-----+\n",
      " |      |summary|               age| name|\n",
      " |      +-------+------------------+-----+\n",
      " |      |  count|                 2|    2|\n",
      " |      |   mean|               3.5| null|\n",
      " |      | stddev|2.1213203435596424| null|\n",
      " |      |    min|                 2|Alice|\n",
      " |      |    25%|                 2| null|\n",
      " |      |    50%|                 2| null|\n",
      " |      |    75%|                 5| null|\n",
      " |      |    max|                 5|  Bob|\n",
      " |      +-------+------------------+-----+\n",
      " |      \n",
      " |      >>> df.summary(\"count\", \"min\", \"25%\", \"75%\", \"max\").show()\n",
      " |      +-------+---+-----+\n",
      " |      |summary|age| name|\n",
      " |      +-------+---+-----+\n",
      " |      |  count|  2|    2|\n",
      " |      |    min|  2|Alice|\n",
      " |      |    25%|  2| null|\n",
      " |      |    75%|  5| null|\n",
      " |      |    max|  5|  Bob|\n",
      " |      +-------+---+-----+\n",
      " |      \n",
      " |      To do a summary for specific columns first select them:\n",
      " |      \n",
      " |      >>> df.select(\"age\", \"name\").summary(\"count\").show()\n",
      " |      +-------+---+----+\n",
      " |      |summary|age|name|\n",
      " |      +-------+---+----+\n",
      " |      |  count|  2|   2|\n",
      " |      +-------+---+----+\n",
      " |      \n",
      " |      See also describe for basic statistics.\n",
      " |      \n",
      " |      .. versionadded:: 2.3.0\n",
      " |  \n",
      " |  tail(self, num)\n",
      " |      Returns the last ``num`` rows as a :class:`list` of :class:`Row`.\n",
      " |      \n",
      " |      Running tail requires moving data into the application's driver process, and doing so with\n",
      " |      a very large ``num`` can crash the driver process with OutOfMemoryError.\n",
      " |      \n",
      " |      >>> df.tail(1)\n",
      " |      [Row(age=5, name='Bob')]\n",
      " |      \n",
      " |      .. versionadded:: 3.0\n",
      " |  \n",
      " |  take(self, num)\n",
      " |      Returns the first ``num`` rows as a :class:`list` of :class:`Row`.\n",
      " |      \n",
      " |      >>> df.take(2)\n",
      " |      [Row(age=2, name='Alice'), Row(age=5, name='Bob')]\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  toDF(self, *cols)\n",
      " |      Returns a new :class:`DataFrame` that with new specified column names\n",
      " |      \n",
      " |      :param cols: list of new column names (string)\n",
      " |      \n",
      " |      >>> df.toDF('f1', 'f2').collect()\n",
      " |      [Row(f1=2, f2='Alice'), Row(f1=5, f2='Bob')]\n",
      " |  \n",
      " |  toJSON(self, use_unicode=True)\n",
      " |      Converts a :class:`DataFrame` into a :class:`RDD` of string.\n",
      " |      \n",
      " |      Each row is turned into a JSON document as one element in the returned RDD.\n",
      " |      \n",
      " |      >>> df.toJSON().first()\n",
      " |      '{\"age\":2,\"name\":\"Alice\"}'\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  toLocalIterator(self, prefetchPartitions=False)\n",
      " |      Returns an iterator that contains all of the rows in this :class:`DataFrame`.\n",
      " |      The iterator will consume as much memory as the largest partition in this\n",
      " |      :class:`DataFrame`. With prefetch it may consume up to the memory of the 2 largest\n",
      " |      partitions.\n",
      " |      \n",
      " |      :param prefetchPartitions: If Spark should pre-fetch the next partition\n",
      " |                                 before it is needed.\n",
      " |      \n",
      " |      >>> list(df.toLocalIterator())\n",
      " |      [Row(age=2, name='Alice'), Row(age=5, name='Bob')]\n",
      " |      \n",
      " |      .. versionadded:: 2.0\n",
      " |  \n",
      " |  transform(self, func)\n",
      " |      Returns a new :class:`DataFrame`. Concise syntax for chaining custom transformations.\n",
      " |      \n",
      " |      :param func: a function that takes and returns a :class:`DataFrame`.\n",
      " |      \n",
      " |      >>> from pyspark.sql.functions import col\n",
      " |      >>> df = spark.createDataFrame([(1, 1.0), (2, 2.0)], [\"int\", \"float\"])\n",
      " |      >>> def cast_all_to_int(input_df):\n",
      " |      ...     return input_df.select([col(col_name).cast(\"int\") for col_name in input_df.columns])\n",
      " |      >>> def sort_columns_asc(input_df):\n",
      " |      ...     return input_df.select(*sorted(input_df.columns))\n",
      " |      >>> df.transform(cast_all_to_int).transform(sort_columns_asc).show()\n",
      " |      +-----+---+\n",
      " |      |float|int|\n",
      " |      +-----+---+\n",
      " |      |    1|  1|\n",
      " |      |    2|  2|\n",
      " |      +-----+---+\n",
      " |      \n",
      " |      .. versionadded:: 3.0\n",
      " |  \n",
      " |  union(self, other)\n",
      " |      Return a new :class:`DataFrame` containing union of rows in this and another\n",
      " |      :class:`DataFrame`.\n",
      " |      \n",
      " |      This is equivalent to `UNION ALL` in SQL. To do a SQL-style set union\n",
      " |      (that does deduplication of elements), use this function followed by :func:`distinct`.\n",
      " |      \n",
      " |      Also as standard in SQL, this function resolves columns by position (not by name).\n",
      " |      \n",
      " |      .. versionadded:: 2.0\n",
      " |  \n",
      " |  unionAll(self, other)\n",
      " |      Return a new :class:`DataFrame` containing union of rows in this and another\n",
      " |      :class:`DataFrame`.\n",
      " |      \n",
      " |      This is equivalent to `UNION ALL` in SQL. To do a SQL-style set union\n",
      " |      (that does deduplication of elements), use this function followed by :func:`distinct`.\n",
      " |      \n",
      " |      Also as standard in SQL, this function resolves columns by position (not by name).\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  unionByName(self, other)\n",
      " |      Returns a new :class:`DataFrame` containing union of rows in this and another\n",
      " |      :class:`DataFrame`.\n",
      " |      \n",
      " |      This is different from both `UNION ALL` and `UNION DISTINCT` in SQL. To do a SQL-style set\n",
      " |      union (that does deduplication of elements), use this function followed by :func:`distinct`.\n",
      " |      \n",
      " |      The difference between this function and :func:`union` is that this function\n",
      " |      resolves columns by name (not by position):\n",
      " |      \n",
      " |      >>> df1 = spark.createDataFrame([[1, 2, 3]], [\"col0\", \"col1\", \"col2\"])\n",
      " |      >>> df2 = spark.createDataFrame([[4, 5, 6]], [\"col1\", \"col2\", \"col0\"])\n",
      " |      >>> df1.unionByName(df2).show()\n",
      " |      +----+----+----+\n",
      " |      |col0|col1|col2|\n",
      " |      +----+----+----+\n",
      " |      |   1|   2|   3|\n",
      " |      |   6|   4|   5|\n",
      " |      +----+----+----+\n",
      " |      \n",
      " |      .. versionadded:: 2.3\n",
      " |  \n",
      " |  unpersist(self, blocking=False)\n",
      " |      Marks the :class:`DataFrame` as non-persistent, and remove all blocks for it from\n",
      " |      memory and disk.\n",
      " |      \n",
      " |      .. note:: `blocking` default has changed to ``False`` to match Scala in 2.0.\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  where = filter(self, condition)\n",
      " |      :func:`where` is an alias for :func:`filter`.\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  withColumn(self, colName, col)\n",
      " |      Returns a new :class:`DataFrame` by adding a column or replacing the\n",
      " |      existing column that has the same name.\n",
      " |      \n",
      " |      The column expression must be an expression over this :class:`DataFrame`; attempting to add\n",
      " |      a column from some other :class:`DataFrame` will raise an error.\n",
      " |      \n",
      " |      :param colName: string, name of the new column.\n",
      " |      :param col: a :class:`Column` expression for the new column.\n",
      " |      \n",
      " |      .. note:: This method introduces a projection internally. Therefore, calling it multiple\n",
      " |          times, for instance, via loops in order to add multiple columns can generate big\n",
      " |          plans which can cause performance issues and even `StackOverflowException`.\n",
      " |          To avoid this, use :func:`select` with the multiple columns at once.\n",
      " |      \n",
      " |      >>> df.withColumn('age2', df.age + 2).collect()\n",
      " |      [Row(age=2, name='Alice', age2=4), Row(age=5, name='Bob', age2=7)]\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  withColumnRenamed(self, existing, new)\n",
      " |      Returns a new :class:`DataFrame` by renaming an existing column.\n",
      " |      This is a no-op if schema doesn't contain the given column name.\n",
      " |      \n",
      " |      :param existing: string, name of the existing column to rename.\n",
      " |      :param new: string, new name of the column.\n",
      " |      \n",
      " |      >>> df.withColumnRenamed('age', 'age2').collect()\n",
      " |      [Row(age2=2, name='Alice'), Row(age2=5, name='Bob')]\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  withWatermark(self, eventTime, delayThreshold)\n",
      " |      Defines an event time watermark for this :class:`DataFrame`. A watermark tracks a point\n",
      " |      in time before which we assume no more late data is going to arrive.\n",
      " |      \n",
      " |      Spark will use this watermark for several purposes:\n",
      " |        - To know when a given time window aggregation can be finalized and thus can be emitted\n",
      " |          when using output modes that do not allow updates.\n",
      " |      \n",
      " |        - To minimize the amount of state that we need to keep for on-going aggregations.\n",
      " |      \n",
      " |      The current watermark is computed by looking at the `MAX(eventTime)` seen across\n",
      " |      all of the partitions in the query minus a user specified `delayThreshold`.  Due to the cost\n",
      " |      of coordinating this value across partitions, the actual watermark used is only guaranteed\n",
      " |      to be at least `delayThreshold` behind the actual event time.  In some cases we may still\n",
      " |      process records that arrive more than `delayThreshold` late.\n",
      " |      \n",
      " |      :param eventTime: the name of the column that contains the event time of the row.\n",
      " |      :param delayThreshold: the minimum delay to wait to data to arrive late, relative to the\n",
      " |          latest record that has been processed in the form of an interval\n",
      " |          (e.g. \"1 minute\" or \"5 hours\").\n",
      " |      \n",
      " |      .. note:: Evolving\n",
      " |      \n",
      " |      >>> sdf.select('name', sdf.time.cast('timestamp')).withWatermark('time', '10 minutes')\n",
      " |      DataFrame[name: string, time: timestamp]\n",
      " |      \n",
      " |      .. versionadded:: 2.1\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  columns\n",
      " |      Returns all column names as a list.\n",
      " |      \n",
      " |      >>> df.columns\n",
      " |      ['age', 'name']\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  dtypes\n",
      " |      Returns all column names and their data types as a list.\n",
      " |      \n",
      " |      >>> df.dtypes\n",
      " |      [('age', 'int'), ('name', 'string')]\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  isStreaming\n",
      " |      Returns ``True`` if this :class:`Dataset` contains one or more sources that continuously\n",
      " |      return data as it arrives. A :class:`Dataset` that reads data from a streaming source\n",
      " |      must be executed as a :class:`StreamingQuery` using the :func:`start` method in\n",
      " |      :class:`DataStreamWriter`.  Methods that return a single answer, (e.g., :func:`count` or\n",
      " |      :func:`collect`) will throw an :class:`AnalysisException` when there is a streaming\n",
      " |      source present.\n",
      " |      \n",
      " |      .. note:: Evolving\n",
      " |      \n",
      " |      .. versionadded:: 2.0\n",
      " |  \n",
      " |  na\n",
      " |      Returns a :class:`DataFrameNaFunctions` for handling missing values.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.1\n",
      " |  \n",
      " |  rdd\n",
      " |      Returns the content as an :class:`pyspark.RDD` of :class:`Row`.\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  schema\n",
      " |      Returns the schema of this :class:`DataFrame` as a :class:`pyspark.sql.types.StructType`.\n",
      " |      \n",
      " |      >>> df.schema\n",
      " |      StructType(List(StructField(age,IntegerType,true),StructField(name,StringType,true)))\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  stat\n",
      " |      Returns a :class:`DataFrameStatFunctions` for statistic functions.\n",
      " |      \n",
      " |      .. versionadded:: 1.4\n",
      " |  \n",
      " |  storageLevel\n",
      " |      Get the :class:`DataFrame`'s current storage level.\n",
      " |      \n",
      " |      >>> df.storageLevel\n",
      " |      StorageLevel(False, False, False, False, 1)\n",
      " |      >>> df.cache().storageLevel\n",
      " |      StorageLevel(True, True, False, True, 1)\n",
      " |      >>> df2.persist(StorageLevel.DISK_ONLY_2).storageLevel\n",
      " |      StorageLevel(True, False, False, False, 2)\n",
      " |      \n",
      " |      .. versionadded:: 2.1\n",
      " |  \n",
      " |  write\n",
      " |      Interface for saving the content of the non-streaming :class:`DataFrame` out into external\n",
      " |      storage.\n",
      " |      \n",
      " |      :return: :class:`DataFrameWriter`\n",
      " |      \n",
      " |      .. versionadded:: 1.4\n",
      " |  \n",
      " |  writeStream\n",
      " |      Interface for saving the content of the streaming :class:`DataFrame` out into external\n",
      " |      storage.\n",
      " |      \n",
      " |      .. note:: Evolving.\n",
      " |      \n",
      " |      :return: :class:`DataStreamWriter`\n",
      " |      \n",
      " |      .. versionadded:: 2.0\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.sql.pandas.map_ops.PandasMapOpsMixin:\n",
      " |  \n",
      " |  mapInPandas(self, func, schema)\n",
      " |      Maps an iterator of batches in the current :class:`DataFrame` using a Python native\n",
      " |      function that takes and outputs a pandas DataFrame, and returns the result as a\n",
      " |      :class:`DataFrame`.\n",
      " |      \n",
      " |      The function should take an iterator of `pandas.DataFrame`\\s and return\n",
      " |      another iterator of `pandas.DataFrame`\\s. All columns are passed\n",
      " |      together as an iterator of `pandas.DataFrame`\\s to the function and the\n",
      " |      returned iterator of `pandas.DataFrame`\\s are combined as a :class:`DataFrame`.\n",
      " |      Each `pandas.DataFrame` size can be controlled by\n",
      " |      `spark.sql.execution.arrow.maxRecordsPerBatch`.\n",
      " |      \n",
      " |      :param func: a Python native function that takes an iterator of `pandas.DataFrame`\\s, and\n",
      " |          outputs an iterator of `pandas.DataFrame`\\s.\n",
      " |      :param schema: the return type of the `func` in PySpark. The value can be either a\n",
      " |          :class:`pyspark.sql.types.DataType` object or a DDL-formatted type string.\n",
      " |      \n",
      " |      >>> from pyspark.sql.functions import pandas_udf\n",
      " |      >>> df = spark.createDataFrame([(1, 21), (2, 30)], (\"id\", \"age\"))\n",
      " |      >>> def filter_func(iterator):\n",
      " |      ...     for pdf in iterator:\n",
      " |      ...         yield pdf[pdf.id == 1]\n",
      " |      >>> df.mapInPandas(filter_func, df.schema).show()  # doctest: +SKIP\n",
      " |      +---+---+\n",
      " |      | id|age|\n",
      " |      +---+---+\n",
      " |      |  1| 21|\n",
      " |      +---+---+\n",
      " |      \n",
      " |      .. seealso:: :meth:`pyspark.sql.functions.pandas_udf`\n",
      " |      \n",
      " |      .. note:: Experimental\n",
      " |      \n",
      " |      .. versionadded:: 3.0\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from pyspark.sql.pandas.map_ops.PandasMapOpsMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.sql.pandas.conversion.PandasConversionMixin:\n",
      " |  \n",
      " |  toPandas(self)\n",
      " |      Returns the contents of this :class:`DataFrame` as Pandas ``pandas.DataFrame``.\n",
      " |      \n",
      " |      This is only available if Pandas is installed and available.\n",
      " |      \n",
      " |      .. note:: This method should only be used if the resulting Pandas's :class:`DataFrame` is\n",
      " |          expected to be small, as all the data is loaded into the driver's memory.\n",
      " |      \n",
      " |      .. note:: Usage with spark.sql.execution.arrow.pyspark.enabled=True is experimental.\n",
      " |      \n",
      " |      >>> df.toPandas()  # doctest: +SKIP\n",
      " |         age   name\n",
      " |      0    2  Alice\n",
      " |      1    5    Bob\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(taxiDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "passenger quantiles: [1.0, 1.0, 1.0, 2.0, 6.0]\n",
      "dist quantiles: [0.0, 1.1, 1.9, 3.7, 47.39]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for column in taxiDF.dtypes:\n",
    "    name = column[0]\n",
    "    colType = column[1]\n",
    "    if colType != 'string' and colType != 'timestamp':\n",
    "        columnQuantiles = taxiDF.approxQuantile(col=name,probabilities=[0.0,0.25,0.50,0.75,1.00],relativeError=0.01)\n",
    "        print(\"{} quantiles: {}\".format(name,columnQuantiles))\n",
    "        stats = [{\n",
    "            \"whislo\": columnQuantiles[0],\n",
    "            \"q1\": columnQuantiles[1],\n",
    "            \"med\": columnQuantiles[2],\n",
    "            \"q3\": columnQuantiles[3],\n",
    "            \"whishi\": columnQuantiles[4]\n",
    "        }]\n",
    "        fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(5,5), sharey=True)\n",
    "        axes.bxp(bxpstats=stats, showfliers=False)\n",
    "        axes.grid(True)\n",
    "        axes.set_title(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 11: Provide an overview over the number of trips per week day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def barchart(dataRows, titleSuffix):\n",
    "    positions = list(reversed(range(len(dataRows))))\n",
    "    names = [str(item[titleSuffix]) + \" (\" + str(item['count']) + \")\" for item in dataRows]\n",
    "    values = [item['count'] for item in dataRows]\n",
    "    plt.grid()\n",
    "    plt.barh(positions,values,align=\"center\")\n",
    "    plt.yticks(positions,names)\n",
    "    plt.xlabel(\"Number of trips\")\n",
    "    plt.title(\"Distribution of trips per \" + titleSuffix)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method_descriptor:\n",
      "\n",
      "weekday(...)\n",
      "    Return the day of the week represented by the date.\n",
      "    Monday == 0 ... Sunday == 6\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(datetime.weekday)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import IntegerType\n",
    "import calendar\n",
    "\n",
    "#udf stands for user defined function\n",
    "@udf \n",
    "def weekdayStr(d):\n",
    "    return calendar.day_name[d.weekday()]\n",
    "\n",
    "@udf(returnType=IntegerType())\n",
    "def weekday(d):\n",
    "    return d.weekday()\n",
    "\n",
    "weekdayRows = taxiDF.select(weekdayStr(taxiDF.dateB).alias(\"weekday\")).groupBy(\"weekday\").count().collect()\n",
    "\n",
    "barchart(weekdayRows, \"weekday\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 12: Provide an overview over the number of trips per hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "@udf(returnType=IntegerType())\n",
    "def hour(d):\n",
    "    return d.hour\n",
    "\n",
    "hourRows = sorted(taxiDF.select(hour(taxiDF.dateB).alias(\"hour\")).groupBy(\"hour\").count().collect())\n",
    "\n",
    "barchart(hourRows, \"hour\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
